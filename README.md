# Performance-of-Ordinary-Least-Squares-for-Autocorrelated-Errors
This is the final project for course STATS 506.
## Overview
Ordinary Least Squares (OLS) is one of the most widely used estimation methods in linear regression. A key assumption for OLS is that the error terms are independent and identically distributed with mean zero and constant variance. In many real-world applications, this assumption is violated because error terms exhibit serial correlation. This report investigates how autocorrelated errors affect the estimation and inference of a simple linear regression model.

Specifically, this report focuses on understanding the behavior of OLS estimators when the error term follows a first-order autoregressive process, AR(1). In particular, this report examines how autocorrelation impact the distribution of regression coefficients and the interval estimations.
